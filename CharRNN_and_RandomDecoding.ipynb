{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jerryson520/NLP-Projects/blob/main/CharRNN_and_RandomDecoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97ab5757",
      "metadata": {
        "id": "97ab5757"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64c0320b",
      "metadata": {
        "id": "64c0320b"
      },
      "source": [
        "### Get the data and process\n",
        "- This is the Mysterious island found in Project Gutenberg."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "50aziqIvG8KE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93388e53-4a1e-4b93-ef8e-f54e3f00633c"
      },
      "id": "50aziqIvG8KE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
        "    text=fp.read()\n",
        "\n",
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFWBD3Xq4-EI",
        "outputId": "19be323c-3b08-463f-a9ae-900c82f15b1a"
      },
      "id": "KFWBD3Xq4-EI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1131478"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4e64a98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4e64a98",
        "outputId": "9c98d731-464c-400d-b354-382c27166a1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "1113093\n",
            "Total Length: 1113062\n",
            "Unique Characters: 83\n"
          ]
        }
      ],
      "source": [
        "## Reading and processing text\n",
        "with open('1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
        "    text=fp.read()\n",
        "# Get the index of 'THE MYSTERIOUS ISLAND' or 'The Mysterious Island'\n",
        "start_indx = text.find('The Mysterious Island')\n",
        "print(start_indx)\n",
        "# Get the index of 'End of the Project Gutenberg' END OF THE PROJECT GUTENBERG\n",
        "end_indx = text.rfind('END OF THE PROJECT GUTENBERG') + len('END OF THE PROJECT GUTENBERG') - 1\n",
        "print(end_indx)\n",
        "# Set text to the text between start and end idx.\n",
        "text = text[start_indx:end_indx+1]\n",
        "# Get the unique set of characters.\n",
        "char_set = set(text)\n",
        "print('Total Length:', len(text))\n",
        "print('Unique Characters:', len(char_set))\n",
        "# assert(len(text) == 1130711)\n",
        "# assert(len(char_set) == 85)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76393bdb",
      "metadata": {
        "id": "76393bdb"
      },
      "source": [
        "### Tokenze and get other helpers\n",
        "- We do this manually since everything is character based."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a445114",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a445114",
        "outputId": "4b8bb48e-31cf-4a19-c219-3f61d43f9c79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text encoded shape:  (1113062,)\n",
            "The Mysterious       == Encoding ==>  [45 60 57  1 38 77 71 72 57 70 61 67 73 71  1]\n",
            "[34 71 64 53 66 56]  == Reverse  ==>  Island\n"
          ]
        }
      ],
      "source": [
        "# The universe of words.\n",
        "chars_sorted = sorted(char_set)\n",
        "\n",
        "# Effectively, these maps are the tokenizer.\n",
        "# Map each char to a unique int. This is a dict.\n",
        "char2int = {char: i for i, char in enumerate(chars_sorted)}\n",
        "# Do the revverse of the above, this should be a np array.\n",
        "int2char = np.array(chars_sorted)\n",
        "\n",
        "# Tokenize the entire corpus. This should be an np array of np.int32 type.\n",
        "text_encoded = np.array([char2int[char] for char in text], dtype = np.int32)\n",
        "\n",
        "print('Text encoded shape: ', text_encoded.shape)\n",
        "\n",
        "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
        "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(int2char[text_encoded[15:21]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d8e0270",
      "metadata": {
        "id": "0d8e0270"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "720cd752",
      "metadata": {
        "id": "720cd752"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2743a57",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2743a57",
        "outputId": "dd949cd8-f049-4dd6-90bf-3f0c6f321c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text encoded shape:  (1113062,)\n",
            "The Mysterious       == Encoding ==>  [45 60 57  1 38 77 71 72 57 70 61 67 73 71  1]\n",
            "[34 71 64 53 66 56]  == Reverse  ==>  Island\n"
          ]
        }
      ],
      "source": [
        "print('Text encoded shape: ', text_encoded.shape)\n",
        "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
        "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(int2char[text_encoded[15:21]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "367e733d",
      "metadata": {
        "id": "367e733d"
      },
      "outputs": [],
      "source": [
        "# assert(\n",
        "#     np.array_equal(\n",
        "#     text_encoded[:15],\n",
        "#         [48, 36, 33, 1, 41, 53, 47, 48, 33, 46, 37, 43, 49, 47,  1]\n",
        "#     )\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cdcafe4",
      "metadata": {
        "id": "2cdcafe4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0c418ca0",
      "metadata": {
        "id": "0c418ca0"
      },
      "source": [
        "### Process the data and get the data loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [1,24,4,5,5,5,6]\n",
        "a[0:4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aoUq-_fKHyj",
        "outputId": "7ec22057-4de3-4b75-f0a4-7dfcc0cfe089"
      },
      "id": "6aoUq-_fKHyj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 24, 4, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f429dc3d",
      "metadata": {
        "id": "f429dc3d"
      },
      "outputs": [],
      "source": [
        "seq_length = 40\n",
        "chunk_size = seq_length + 1\n",
        "\n",
        "# Break up the data into chunks of size 41. This should be a list of lists.\n",
        "# Use text_encoded. This will be used to get (x, y) pairs.\n",
        "text_chunks = [text_encoded[i:i+chunk_size] for i in range(len(text_encoded) - seq_length)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_chunks[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sywCfVMoOODX",
        "outputId": "04bf408b-e838-4e67-a87c-061660514e37"
      },
      "id": "sywCfVMoOODX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e329fffd",
      "metadata": {
        "id": "e329fffd"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the text chunk at index idx.\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        # Return (x, y) where x has length 40 and y has length 40.\n",
        "        # y should be x shifted by 1 time.\n",
        "        return (text_chunk[0:-1], text_chunk[1:])\n",
        "\n",
        "seq_dataset = TextDataset(torch.tensor(np.array(text_chunks))) # batch_size * seq_len"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0FsyBzKN1MhM"
      },
      "id": "0FsyBzKN1MhM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71328555",
      "metadata": {
        "id": "71328555",
        "outputId": "6a312c0c-06a2-4ce2-e9a5-639235fe7471",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([40]) torch.Size([40])\n",
            "Input (x): 'The Mysterious Island, by Jules Verne\\n\\nT'\n",
            "Target (y): 'he Mysterious Island, by Jules Verne\\n\\nTh'\n",
            "\n",
            "torch.Size([40]) torch.Size([40])\n",
            "Input (x): 'he Mysterious Island, by Jules Verne\\n\\nTh'\n",
            "Target (y): 'e Mysterious Island, by Jules Verne\\n\\nThi'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, (seq, target) in enumerate(seq_dataset):\n",
        "    # 40 characters for source and target ...\n",
        "    print(seq.shape, target.shape)\n",
        "    print('Input (x):', repr(''.join(int2char[seq])))\n",
        "    print('Target (y):', repr(''.join(int2char[target])))\n",
        "    print()\n",
        "    if i == 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebb989c3",
      "metadata": {
        "id": "ebb989c3"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a881b316",
      "metadata": {
        "id": "a881b316"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "torch.manual_seed(1)\n",
        "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f77f7f8",
      "metadata": {
        "id": "0f77f7f8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "45ed0b2f",
      "metadata": {
        "id": "45ed0b2f"
      },
      "source": [
        "### Write the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b4cbf1e",
      "metadata": {
        "id": "1b4cbf1e"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        # Set to an embedding layer of vocab_size by embed_dim.\n",
        "        self.embedding = nn.Embedding(\n",
        "            vocab_size,\n",
        "            embed_dim,\n",
        "            padding_idx=None\n",
        "        )\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        # Set to an LSTM with x having embed_dim and h dimension rnn_hidden_size.\n",
        "        # batch_first should be true.\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size = embed_dim,\n",
        "            hidden_size = rnn_hidden_size,\n",
        "            batch_first = True\n",
        "        )\n",
        "\n",
        "        # Make a linear layer from rnn_hidden_size to vocab_size.\n",
        "        # This will be used to get the yt for each xt.\n",
        "        self.fc = nn.Linear(\n",
        "            in_features = self.rnn_hidden_size,\n",
        "            out_features = vocab_size)\n",
        "\n",
        "    def forward(self, text, hidden=None, cell=None):\n",
        "        # Get the embeddings for text.\n",
        "        out = self.embedding(text) # batch_size * seq_len * embed_dim\n",
        "\n",
        "        # Pass out, hidden and cell through the rnn.\n",
        "        # If hidden is None, don't specify it and just use out.\n",
        "        if hidden is not None:\n",
        "            out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        else:\n",
        "            out, (hidden, cell) = self.rnn(out) # out: batch * seq_len * hidden_size\n",
        "\n",
        "        # Pass out through fc.\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, (hidden, cell)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initialize to zeros of 1 by ??? appropriate dimensions.\n",
        "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        return hidden.to(device), cell.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f16c03dc",
      "metadata": {
        "id": "f16c03dc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "00789dfd",
      "metadata": {
        "id": "00789dfd"
      },
      "source": [
        "### Do this right way - across all data all at once!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33380607",
      "metadata": {
        "id": "33380607",
        "outputId": "4906ea22-ab9b-42e5-e2e6-18fd104d443e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(83, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "vocab_size = len(int2char)\n",
        "embed_dim = 256\n",
        "rnn_hidden_size = 512\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
        "model = model.to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f47f48a",
      "metadata": {
        "id": "2f47f48a",
        "outputId": "c259d934-e8c2-4e7c-b5a2-562bcb6903fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 1.8589\n",
            "Epoch 10 loss: 1.9141\n",
            "Epoch 20 loss: 1.8323\n",
            "Epoch 30 loss: 1.7749\n",
            "Epoch 40 loss: 1.7208\n",
            "Epoch 50 loss: 1.7147\n",
            "Epoch 60 loss: 1.6634\n",
            "Epoch 70 loss: 1.6843\n",
            "Epoch 80 loss: 1.5436\n",
            "Epoch 90 loss: 1.5359\n",
            "Epoch 100 loss: 1.5601\n",
            "Epoch 110 loss: 1.5613\n",
            "Epoch 120 loss: 1.5092\n",
            "Epoch 130 loss: 1.5445\n",
            "Epoch 140 loss: 1.4979\n",
            "Epoch 150 loss: 1.5395\n",
            "Epoch 160 loss: 1.5086\n",
            "Epoch 170 loss: 1.5039\n",
            "Epoch 180 loss: 1.4999\n",
            "Epoch 190 loss: 1.4859\n",
            "Epoch 200 loss: 1.4746\n",
            "Epoch 210 loss: 1.4383\n",
            "Epoch 220 loss: 1.4343\n",
            "Epoch 230 loss: 1.4839\n",
            "Epoch 240 loss: 1.4800\n",
            "Epoch 250 loss: 1.4152\n",
            "Epoch 260 loss: 1.4137\n",
            "Epoch 270 loss: 1.4180\n",
            "Epoch 280 loss: 1.4707\n",
            "Epoch 290 loss: 1.4234\n",
            "Epoch 300 loss: 1.3895\n",
            "Epoch 310 loss: 1.3773\n",
            "Epoch 320 loss: 1.4086\n",
            "Epoch 330 loss: 1.4408\n",
            "Epoch 340 loss: 1.3993\n",
            "Epoch 350 loss: 1.4153\n",
            "Epoch 360 loss: 1.3527\n",
            "Epoch 370 loss: 1.3755\n",
            "Epoch 380 loss: 1.3338\n",
            "Epoch 390 loss: 1.3603\n",
            "Epoch 400 loss: 1.3362\n",
            "Epoch 410 loss: 1.3795\n",
            "Epoch 420 loss: 1.3096\n",
            "Epoch 430 loss: 1.3705\n",
            "Epoch 440 loss: 1.3734\n",
            "Epoch 450 loss: 1.3003\n",
            "Epoch 460 loss: 1.3075\n",
            "Epoch 470 loss: 1.3412\n",
            "Epoch 480 loss: 1.3484\n",
            "Epoch 490 loss: 1.3375\n",
            "Epoch 500 loss: 1.3685\n",
            "Epoch 510 loss: 1.3590\n",
            "Epoch 520 loss: 1.4012\n",
            "Epoch 530 loss: 1.3708\n",
            "Epoch 540 loss: 1.3247\n",
            "Epoch 550 loss: 1.3512\n",
            "Epoch 560 loss: 1.3688\n",
            "Epoch 570 loss: 1.3076\n",
            "Epoch 580 loss: 1.3301\n",
            "Epoch 590 loss: 1.3879\n",
            "Epoch 600 loss: 1.3565\n",
            "Epoch 610 loss: 1.3340\n",
            "Epoch 620 loss: 1.2621\n",
            "Epoch 630 loss: 1.2702\n",
            "Epoch 640 loss: 1.3320\n",
            "Epoch 650 loss: 1.2932\n",
            "Epoch 660 loss: 1.3122\n",
            "Epoch 670 loss: 1.3176\n",
            "Epoch 680 loss: 1.2816\n",
            "Epoch 690 loss: 1.2798\n",
            "Epoch 700 loss: 1.3149\n",
            "Epoch 710 loss: 1.3344\n",
            "Epoch 720 loss: 1.3029\n",
            "Epoch 730 loss: 1.3350\n",
            "Epoch 740 loss: 1.2954\n",
            "Epoch 750 loss: 1.2581\n",
            "Epoch 760 loss: 1.2267\n",
            "Epoch 770 loss: 1.3207\n",
            "Epoch 780 loss: 1.3015\n",
            "Epoch 790 loss: 1.2688\n",
            "Epoch 800 loss: 1.2137\n",
            "Epoch 810 loss: 1.2739\n",
            "Epoch 820 loss: 1.2693\n",
            "Epoch 830 loss: 1.2205\n",
            "Epoch 840 loss: 1.2892\n",
            "Epoch 850 loss: 1.2625\n",
            "Epoch 860 loss: 1.2950\n",
            "Epoch 870 loss: 1.2531\n",
            "Epoch 880 loss: 1.3333\n",
            "Epoch 890 loss: 1.3282\n",
            "Epoch 900 loss: 1.2658\n",
            "Epoch 910 loss: 1.3035\n",
            "Epoch 920 loss: 1.2960\n",
            "Epoch 930 loss: 1.2255\n",
            "Epoch 940 loss: 1.2412\n",
            "Epoch 950 loss: 1.3188\n",
            "Epoch 960 loss: 1.3263\n",
            "Epoch 970 loss: 1.2319\n",
            "Epoch 980 loss: 1.2263\n",
            "Epoch 990 loss: 1.3113\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "# Set to 10000.\n",
        "num_epochs = 10000\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# epochs here will mean batches.\n",
        "# If the above takes too long, use 1000.\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    hidden, cell = model.init_hidden(batch_size)\n",
        "\n",
        "    # Get the next batch from seq_dl\n",
        "    seq_batch, target_batch = next(iter(seq_dl))\n",
        "\n",
        "    seq_batch = seq_batch.to(device) # batch_size * seq_len\n",
        "    target_batch = target_batch.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # Pass through the model.\n",
        "    logits, _ = model(seq_batch, hidden, cell) # batch_size * seq_len * vocab_size\n",
        "\n",
        "    # Get the loss.\n",
        "    # You'll need to reshape / view things to make this work.\n",
        "    loss += criterion(logits.view(logits.shape[0] * logits.shape[1], -1), target_batch.long().view(-1))\n",
        "\n",
        "    # Do back prop.\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Get the value in the tensor loss.\n",
        "    loss = loss.item()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch} loss: {loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17af6f8e",
      "metadata": {
        "id": "17af6f8e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f398f67",
      "metadata": {
        "id": "0f398f67",
        "outputId": "720860dc-a8c1-4b47-c72a-951f454c9671",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities: tensor([[0.0159, 0.1173, 0.8668]])\n",
            "[[1]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [1]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]]\n"
          ]
        }
      ],
      "source": [
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "logits = torch.tensor([[-1.0, 1.0, 3.0]])\n",
        "# print(logits.shape)\n",
        "\n",
        "# Get the probabilities for these logits.\n",
        "print('Probabilities:', nn.Softmax()(logits))\n",
        "\n",
        "# Get a Categorical random variable with the above probabilities for each of the classes.\n",
        "m = Categorical(nn.Softmax()(logits))\n",
        "# Generate 10 things.\n",
        "samples = m.sample((10,))\n",
        "\n",
        "print(samples.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81ec176d",
      "metadata": {
        "id": "81ec176d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "214de36c-3f6d-4152-c69b-c03469064a7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3],\n",
              "       [5],\n",
              "       [7]])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "a = np.array([[1,2,3], [3,4,5], [5,6,7]])\n",
        "a[:,-1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0547467d",
      "metadata": {
        "id": "0547467d"
      },
      "source": [
        "### Random decoding.\n",
        "- This compounds problems: once you make a mistake, you can't undo it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "614fb236",
      "metadata": {
        "id": "614fb236",
        "outputId": "4c3a85d2-99fb-4a65-c875-42617d73c3f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The island,\n",
            "my forewhat game, as iron foliar raiser!\n",
            "\n",
            "“You are there wase from an island, without the\n",
            "planks. All hours in more was finished to the sould\n",
            "have been energue to Granite, that the\n",
            "from evenable sufficient\n",
            "gliment he\n",
            "did, that an one rocument. It has been seni.\n",
            "\n",
            "Last Pencroft, he talking them back.”\n",
            "\n",
            "“Consider 4\n",
            "\n",
            "Cyrus\n",
            "Harding, “I doubt nothing apmeriantly have explorations, among, the crusion; the jokers\n",
            "on the poor for another\n",
            "BOb Harding looking and for thoust could not an” confident, in th\n"
          ]
        }
      ],
      "source": [
        "def random_sample(\n",
        "    model,\n",
        "    starting_str,\n",
        "    len_generated_text=500,\n",
        "):\n",
        "\n",
        "    # Encode starting string into a tensor using char2str.\n",
        "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
        "\n",
        "    # Reshape to be 1 by ??? - let PyTorch figure this out.\n",
        "    encoded_input = encoded_input.view(1,-1)\n",
        "\n",
        "    # This will be what you generate, but it starts off with something.\n",
        "    generated_str = starting_str\n",
        "\n",
        "    # Put model in eval mode. This matters if we had dropout o batch / layer norms.\n",
        "    model.eval()\n",
        "\n",
        "    hidden, cell = model.init_hidden(1) # 1 * batch_size * hidden_size\n",
        "\n",
        "    hidden = hidden.to(device)\n",
        "\n",
        "    cell = cell.to(device)\n",
        "\n",
        "    # Build up the starting hidden and cell states.\n",
        "    # You can do this all in one go?\n",
        "    for c in range(len(starting_str)-1):\n",
        "        # Feed each letter 1 by 1 and then get the final hidden state.\n",
        "        out = encoded_input[:,c].view(1,1)\n",
        "        # Pass out through, note we update hidden and cell and use them again\n",
        "        # print(out.shape)\n",
        "        # print(hidden.shape)\n",
        "        # print(cell.shape)\n",
        "        _, (hidden, cell) = model(out, hidden, cell)\n",
        "\n",
        "    # Gte the last char; note we did not do go to the last char above.\n",
        "    last_char = encoded_input[:,-1].view(1,1) # 1 * 1\n",
        "    # Generate chars one at a time, add them to generated_str.\n",
        "    # Do this over and over until you get the desired length.\n",
        "    for i in range(len_generated_text):\n",
        "\n",
        "        # Use hidden and cell from the above.\n",
        "        # Use last_char, which will be updated over and over.\n",
        "        # print(last_char.view(1,1) == last_char)\n",
        "        logits, (hidden, cell) = model(last_char, hidden, cell) # 1 * vocab_size\n",
        "\n",
        "        # Get the logits.\n",
        "        logits = torch.squeeze(logits, dim = 0)\n",
        "\n",
        "\n",
        "        # m is a random variable with probabilities based on the softmax of the logits.\n",
        "        m = Categorical(nn.Softmax()(logits))\n",
        "\n",
        "        # Generate from m 1 char.\n",
        "        last_char = m.sample((1,))\n",
        "        # print(last_char.shape)\n",
        "\n",
        "        # Add the generated char to generated_str, but pass it through int2str so that\n",
        "        generated_str += int2char[last_char.item()]\n",
        "\n",
        "    return generated_str\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model.to(device)\n",
        "print(random_sample(model, starting_str='The island'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qEnIPWyYfPkl"
      },
      "id": "qEnIPWyYfPkl",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}